{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Zero-fault-shot learning for bearing spall type classification by hybrid approach**\n",
    "\n",
    "In this notebook the spall type of the processed signals is predicted using a fully connected feed-forward neural network. The notebook loads the signals after the physical domain adaptaion, where they were processed using the \"main\" matlab code.\n",
    "\n",
    "In the first code block the notebook imports several libraries where in the second block new functions are defined. In the third block the data is loaded. In the fourth block the fully connected feed-forward neural network is set and its training function is presented. The training phase and the test pashe are presented in the fifth and six code blocks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1658230611713,
     "user": {
      "displayName": "עמרי מתניה",
      "userId": "10129715939663802950"
     },
     "user_tz": -180
    },
    "id": "aMcQJUlY8crm"
   },
   "outputs": [],
   "source": [
    "# First code block - Import Python libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "import scipy.io\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second code block - functions\n",
    "\n",
    "def classification_acc(y_pred, y_true):\n",
    "    y_pred_tag = torch.softmax(y_pred, dim=1).argmax(dim=1)\n",
    "    y_true_tag = torch.softmax(y_true, dim=1).argmax(dim=1)\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_true_tag).float().sum()\n",
    "    acc = correct_results_sum/float(y_true.size(0))*100\n",
    "    return acc\n",
    "    \n",
    "class SimpleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        #pdb.set_trace()\n",
    "        #self.X_data = torch.unsqueeze(X_data, dim=1)\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "def convert_X_y_mat2py(X, y):\n",
    "    \n",
    "    X = X.transpose()\n",
    "    y = y.transpose()\n",
    "    y = y.squeeze()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1658230941536,
     "user": {
      "displayName": "עמרי מתניה",
      "userId": "10129715939663802950"
     },
     "user_tz": -180
    },
    "id": "gFN8-tfns6-W",
    "outputId": "bb1c6906-ef80-4309-9bf8-7cdd8337cf7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning / Validation sizes: 10126/4340\n",
      "Validation dataset CWRU size: 181\n",
      "Validation dataset PU size: 238\n",
      "Validation dataset XJTU_SY size: 1563\n",
      "Validation dataset PRONOSTIA-FEMTO size: 43\n",
      "Validation dataset NBSWT size: 50\n",
      "Test size: 17\n"
     ]
    }
   ],
   "source": [
    "# Third code block - data loading\n",
    "BATCH_SIZE = 64\n",
    "test_set_name = 'MFPT' # 'MFPT', 'PU', 'XJTU_SY', 'PRONOSTIA-FEMTO', 'NBSWT'\n",
    "datasets_names = ['CWRU', 'MFPT', 'PU', 'XJTU_SY', 'PRONOSTIA-FEMTO', 'NBSWT']\n",
    "data_path = 'E:\\\\data\\\\papers\\\\zero_fault_shot_learning\\\\datasets'\n",
    "processed_data = scipy.io.loadmat(data_path + '\\\\processed_data.mat')\n",
    "\n",
    "X_training = processed_data[\"X_training\"]\n",
    "y_training = processed_data[\"y_training\"]\n",
    "X_training, y_training = convert_X_y_mat2py(X_training, y_training)\n",
    "\n",
    "X_CWRU = processed_data[\"X_CWRU\"]\n",
    "y_CWRU = processed_data[\"y_CWRU\"]\n",
    "X_CWRU, y_CWRU = convert_X_y_mat2py(X_CWRU, y_CWRU)\n",
    "\n",
    "X_MFPT = processed_data[\"X_MFPT\"]\n",
    "y_MFPT = processed_data[\"y_MFPT\"]\n",
    "X_MFPT, y_MFPT = convert_X_y_mat2py(X_MFPT, y_MFPT)\n",
    "\n",
    "X_PU = processed_data[\"X_PU\"]\n",
    "y_PU = processed_data[\"y_PU\"]\n",
    "X_PU, y_PU = convert_X_y_mat2py(X_PU, y_PU)\n",
    "\n",
    "X_XJTU_SY = processed_data[\"X_XJTU_SY\"]\n",
    "y_XJTU_SY = processed_data[\"y_XJTU_SY\"]\n",
    "X_XJTU_SY, y_XJTU_SY = convert_X_y_mat2py(X_XJTU_SY, y_XJTU_SY)\n",
    "\n",
    "X_PRONOSTIA_FEMTO = processed_data[\"X_PRONOSTIA_FEMTO\"]\n",
    "y_PRONOSTIA_FEMTO = processed_data[\"y_PRONOSTIA_FEMTO\"]\n",
    "X_PRONOSTIA_FEMTO, y_PRONOSTIA_FEMTO = convert_X_y_mat2py(X_PRONOSTIA_FEMTO, y_PRONOSTIA_FEMTO)\n",
    "\n",
    "X_NBSWT = processed_data[\"X_NBSWT\"]\n",
    "y_NBSWT = processed_data[\"y_NBSWT\"]\n",
    "X_NBSWT, y_NBSWT = convert_X_y_mat2py(X_NBSWT, y_NBSWT)\n",
    "\n",
    "X_training, y_training = shuffle(X_training, y_training, random_state=0)\n",
    "X_training, X_val, y_training, y_val = train_test_split(X_training, y_training, \n",
    "                                                        test_size=0.3, random_state=42)\n",
    "\n",
    "training_data = SimpleDataset(torch.FloatTensor(X_training), torch.FloatTensor(y_training))\n",
    "val_data = SimpleDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "CWRU_data = SimpleDataset(torch.FloatTensor(X_CWRU), torch.FloatTensor(y_CWRU))\n",
    "MFPT_data = SimpleDataset(torch.FloatTensor(X_MFPT), torch.FloatTensor(y_MFPT))\n",
    "PU_data = SimpleDataset(torch.FloatTensor(X_PU), torch.FloatTensor(y_PU))\n",
    "XJTU_SY_data = SimpleDataset(torch.FloatTensor(X_XJTU_SY), torch.FloatTensor(y_XJTU_SY))\n",
    "PRONOSTIA_FEMTO_data = SimpleDataset(torch.FloatTensor(X_PRONOSTIA_FEMTO), torch.FloatTensor(y_PRONOSTIA_FEMTO))\n",
    "NBSWT_data = SimpleDataset(torch.FloatTensor(X_NBSWT), torch.FloatTensor(y_NBSWT))\n",
    "\n",
    "datasets_list = [CWRU_data, MFPT_data, PU_data, XJTU_SY_data, PRONOSTIA_FEMTO_data, NBSWT_data]\n",
    "val_datasets_data_list = []\n",
    "val_datasets_names = []\n",
    "for ii in range(len(datasets_list)):\n",
    "    if datasets_names[ii] != test_set_name:\n",
    "        val_datasets_data_list.append(datasets_list[ii])\n",
    "        val_datasets_names.append(datasets_names[ii])\n",
    "    else:\n",
    "        test_data = datasets_list[ii]\n",
    "\n",
    "## Creating dataloaders\n",
    "training_loader = DataLoader(dataset=training_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "val_datasets_loader_list = []\n",
    "for ii in range(len(val_datasets_data_list)):\n",
    "    val_datasets_loader_list.append(DataLoader(dataset=val_datasets_data_list[ii], \n",
    "                                       batch_size=BATCH_SIZE, shuffle=False))\n",
    "    \n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Trainning / Validation sizes: \" + str(len(training_data)) + \"/\" + str(len(val_data)))\n",
    "for ii in range(len(val_datasets_data_list)):\n",
    "    print(\"Validation dataset \" + val_datasets_names[ii] + \" size: \" + \n",
    "          str(len(val_datasets_data_list[ii])))\n",
    "print(\"Test size: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35217,
     "status": "ok",
     "timestamp": 1658230978189,
     "user": {
      "displayName": "עמרי מתניה",
      "userId": "10129715939663802950"
     },
     "user_tz": -180
    },
    "id": "nSUOUrsB0h1C",
    "outputId": "82038f0b-c5bb-41fc-f384-a2c6cb7656e7"
   },
   "outputs": [],
   "source": [
    "# Fourth code block - fully connected feed-forward neural network \n",
    "class classModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(classModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 10)  # input is (x,y)\n",
    "        self.fc2 = nn.Linear(10, 10)  # input is (x,y)\n",
    "        self.fc3 = nn.Linear(10, 10)  # input is (x,y)\n",
    "        self.fc4 = nn.Linear(10, 4)\n",
    "\n",
    "        #self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Feature Extractor part\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = F.softmax(self.fc4(x))\n",
    "        \n",
    "        return pred\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, class_model, LEARNING_RATE, device=\"cpu\"):\n",
    "        self.class_model = class_model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.optimizer = optim.Adam(self.class_model.parameters(), lr=LEARNING_RATE)\n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, data_loader, eval_data_loader, EPOCHS):   \n",
    "\n",
    "        self.class_model.zero_grad()\n",
    "        self.class_model.train()\n",
    "        \n",
    "        ## Training Epochs\n",
    "        for e in range(1, EPOCHS+1):\n",
    "            epoch_bce_loss = 0\n",
    "            epoch_acc = 0\n",
    "\n",
    "            ## Mini-bacth Handling\n",
    "            for X, y in data_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                y = nn.functional.one_hot(y.long(), num_classes=4).float()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                pred = self.class_model(X)\n",
    "\n",
    "                # Loss calculations\n",
    "                bce_loss = self.criterion(pred, y)                        \n",
    "                loss = bce_loss\n",
    "                acc = classification_acc(pred, y)\n",
    "\n",
    "                # Back-propagation\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Metrics update\n",
    "                epoch_bce_loss += bce_loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "            print(f'Epoch {e+0:03}: | BCE Loss: {epoch_bce_loss/len(data_loader):.5f} | Train Acc: {epoch_acc/len(data_loader):.3f}')\n",
    "\n",
    "            ## Evaluating on Target\n",
    "            self.evaluate(eval_data_loader, data_type=\"Val\")\n",
    "\n",
    "    def evaluate(self, data_loader, data_type=\"Val\"):\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        self.class_model.eval()\n",
    "        batch_sizes = []\n",
    "        \n",
    "        ## Iterate over all dataset\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_batch = nn.functional.one_hot(y_batch.long(), num_classes=4).float()\n",
    "                \n",
    "                y_pred = self.class_model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                acc = classification_acc(y_pred, y_batch)\n",
    "                \n",
    "                batch_sizes.append(len(y_batch))\n",
    "                \n",
    "                test_loss += (len(y_batch) / batch_sizes[0]) * loss.item()\n",
    "                test_acc += (len(y_batch) / batch_sizes[0]) * acc.item()\n",
    "                \n",
    "        nrml_factor = sum(batch_sizes) / batch_sizes[0]\n",
    "\n",
    "        print(data_type + f' | Loss: {test_loss/nrml_factor:.5f} | Acc: {test_acc/nrml_factor:.3f}')\n",
    "        \n",
    "        return test_acc/nrml_factor\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        data = torch.empty((1,2))\n",
    "        #pdb.set_trace()\n",
    "        preds = torch.empty((0))\n",
    "        y_real = torch.empty((0))\n",
    "        self.class_model.eval()\n",
    "        \n",
    "        ## Iterate over all dataset\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                y_pred = self.class_model(X_batch)\n",
    "\n",
    "                y_pred_tag = torch.softmax(y_pred, dim=1).argmax(dim=1)\n",
    "\n",
    "                #data = torch.cat((data, X_batch))\n",
    "                preds = torch.cat((preds, y_pred_tag))\n",
    "                y_real = torch.cat((y_real, y_batch))\n",
    "\n",
    "        return preds, y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1658153092411,
     "user": {
      "displayName": "עמרי מתניה",
      "userId": "10129715939663802950"
     },
     "user_tz": -180
    },
    "id": "vwVg1rknAuKo",
    "outputId": "90b59480-cbd0-4956-b6e6-4775424bc43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | BCE Loss: 1.22814 | Train Acc: 80.130\n",
      "Val | Loss: 0.83989 | Acc: 100.000\n",
      "Epoch 002: | BCE Loss: 0.76254 | Train Acc: 100.000\n",
      "Val | Loss: 0.74727 | Acc: 100.000\n",
      "Epoch 003: | BCE Loss: 0.74594 | Train Acc: 100.000\n",
      "Val | Loss: 0.74495 | Acc: 100.000\n",
      "Epoch 004: | BCE Loss: 0.74467 | Train Acc: 100.000\n",
      "Val | Loss: 0.74433 | Acc: 100.000\n",
      "Epoch 005: | BCE Loss: 0.74425 | Train Acc: 100.000\n",
      "Val | Loss: 0.74408 | Acc: 100.000\n",
      "Epoch 006: | BCE Loss: 0.74405 | Train Acc: 100.000\n",
      "Val | Loss: 0.74395 | Acc: 100.000\n",
      "Epoch 007: | BCE Loss: 0.74394 | Train Acc: 100.000\n",
      "Val | Loss: 0.74387 | Acc: 100.000\n",
      "Epoch 008: | BCE Loss: 0.74387 | Train Acc: 100.000\n",
      "Val | Loss: 0.74382 | Acc: 100.000\n",
      "Epoch 009: | BCE Loss: 0.74383 | Train Acc: 100.000\n",
      "Val | Loss: 0.74379 | Acc: 100.000\n",
      "Epoch 010: | BCE Loss: 0.74380 | Train Acc: 100.000\n",
      "Val | Loss: 0.74377 | Acc: 100.000\n",
      "Epoch 011: | BCE Loss: 0.74377 | Train Acc: 100.000\n",
      "Val | Loss: 0.74375 | Acc: 100.000\n",
      "Epoch 012: | BCE Loss: 0.74375 | Train Acc: 100.000\n",
      "Val | Loss: 0.74374 | Acc: 100.000\n",
      "Epoch 013: | BCE Loss: 0.74374 | Train Acc: 100.000\n",
      "Val | Loss: 0.74373 | Acc: 100.000\n",
      "Epoch 014: | BCE Loss: 0.74373 | Train Acc: 100.000\n",
      "Val | Loss: 0.74372 | Acc: 100.000\n",
      "Epoch 015: | BCE Loss: 0.74372 | Train Acc: 100.000\n",
      "Val | Loss: 0.74371 | Acc: 100.000\n",
      "Epoch 016: | BCE Loss: 0.74371 | Train Acc: 100.000\n",
      "Val | Loss: 0.74370 | Acc: 100.000\n",
      "Epoch 017: | BCE Loss: 0.74371 | Train Acc: 100.000\n",
      "Val | Loss: 0.74370 | Acc: 100.000\n",
      "Epoch 018: | BCE Loss: 0.74370 | Train Acc: 100.000\n",
      "Val | Loss: 0.74370 | Acc: 100.000\n",
      "Epoch 019: | BCE Loss: 0.74370 | Train Acc: 100.000\n",
      "Val | Loss: 0.74369 | Acc: 100.000\n",
      "Epoch 020: | BCE Loss: 0.74370 | Train Acc: 100.000\n",
      "Val | Loss: 0.74369 | Acc: 100.000\n",
      "\n",
      "CWRU | Loss: 0.75519 | Acc: 98.895\n",
      "PU | Loss: 0.74369 | Acc: 100.000\n",
      "XJTU_SY | Loss: 0.76244 | Acc: 98.720\n",
      "PRONOSTIA-FEMTO | Loss: 0.74367 | Acc: 100.000\n",
      "NBSWT | Loss: 0.76767 | Acc: 98.000\n"
     ]
    }
   ],
   "source": [
    "# Fifth code block - training phase\n",
    "torch.manual_seed(10)\n",
    "\n",
    "## Train parameters\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "device = 'cpu'\n",
    "\n",
    "## Creating model\n",
    "input_dim = X_training.shape[1]\n",
    "class_model = classModel(input_dim)\n",
    "class_model.to(device)\n",
    "\n",
    "## Training model\n",
    "trainer = Trainer(class_model, LEARNING_RATE)\n",
    "trainer.train(training_loader, val_loader, EPOCHS)\n",
    "\n",
    "print()\n",
    "for ii in range(len(val_datasets_loader_list)):\n",
    "    trainer.evaluate(val_datasets_loader_list[ii], val_datasets_names[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | Loss: 0.74372 | Acc: 100.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# six code block - test phase\n",
    "\n",
    "trainer.evaluate(test_loader, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_NBSWT # ['CWRU', 'MFPT', 'PU', 'XJTU_SY', 'PRONOSTIA-FEMTO', 'NBSWT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prd = trainer.predict(val_datasets_loader_list[4]) # val_datasets_loader_list test_loader\n",
    "y_prd = y_prd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[49  1]\n",
      " [ 0  0]]\n"
     ]
    }
   ],
   "source": [
    "#y_test = [1, 0, 1, 1, 0, 1, 0, 0]\n",
    "#y_prd = [1, 0, 1, 1, 0, 0, 1, 0]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_prd)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPHtU0gh6kWRyBv2lJQGRfk",
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
